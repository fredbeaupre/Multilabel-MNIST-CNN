{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project3_finalcopy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Mrgcq9Irvm"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh1ddcfYQf79"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Reshape, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJh_f3lpFDd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afa30fe-7bd5-4654-bed9-016828764569"
      },
      "source": [
        "# mount your drive, may need to force remount if not working\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-hKXo2WFFt6",
        "outputId": "3e1eb412-4d1a-4a2d-b765-7d4daae9aeac"
      },
      "source": [
        "# for this to work, first upload the .h5 file into your drive and copy the path, then change the path name variable \n",
        "fred_path = '/content/drive/MyDrive/MNIST_synthetic.h5'\n",
        "will_path = '/content/drive/MyDrive/MNIST_synthetic.h5'\n",
        "jess_path = '/content/drive/MyDrive/School/COMP 551/COMP 551 Mini Project 3/MNIST_synthetic.h5'\n",
        "\n",
        "with h5py.File(fred_path, 'r') as hdfid:\n",
        "     print(hdfid.keys())\n",
        "\n",
        "     test_dataset  = np.array(hdfid['test_dataset'][()])\n",
        "     train_dataset  = np.array(hdfid['train_dataset'][()])\n",
        "     train_labels  = np.array(hdfid['train_labels'][()])\n",
        "\n",
        "train_onehot_labels = to_categorical(train_labels)\n",
        "# reshaping for visualization \n",
        "train = train_dataset.reshape(train_dataset.shape[0], 64, 64)\n",
        "test = test_dataset.reshape(test_dataset.shape[0], 64, 64)\n",
        "\n",
        "train.shape, test.shape, train_dataset.shape, test_dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<KeysViewHDF5 ['test_dataset', 'train_dataset', 'train_labels']>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((56000, 64, 64), (14000, 64, 64), (56000, 64, 64, 1), (14000, 64, 64, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WRax3rqIwBD"
      },
      "source": [
        "# Pre-processing to get cropped images\n",
        "# cutting off the top and bottom\n",
        "new_cut = []\n",
        "for datapoint in train: \n",
        "  new_cut.append(datapoint[20:40,:])\n",
        "\n",
        "new_cut_test = []\n",
        "for datapoint in test: \n",
        "  new_cut_test.append(datapoint[20:40,:])\n",
        "\n",
        "new_cut = np.array(new_cut)\n",
        "new_cut_test = np.array(new_cut_test)\n",
        "\n",
        "new_cut = new_cut.reshape(new_cut.shape[0], 20, 64, 1)\n",
        "new_cut_test = new_cut_test.reshape(new_cut_test.shape[0], 20, 64, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufY23xcBF6wH"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zb6LvI0FLQT"
      },
      "source": [
        "# from softmax probabilities to class predictions of form [0, 0, 0, 0, 0]\n",
        "def evaluate(yhat):\n",
        "  results = []\n",
        "  for y in yhat:\n",
        "    current_datapoint = []\n",
        "    for val in y:\n",
        "      current_datapoint.append(np.argmax(val))\n",
        "    results.append(current_datapoint)\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9OjTC5yF_y9"
      },
      "source": [
        "# creates output csv with 'id' and 'label' columns\n",
        "# include path name of file you want to save it in \n",
        "def id_label_df(yhat, file_path):\n",
        "  results = evaluate(yhat)\n",
        "  df = pd.DataFrame(results, columms = ['Label'])\n",
        "  df.to_csv(file_path + '/out.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZaX6H3nGAHp"
      },
      "source": [
        "# k fold validation\n",
        "def kfold_evaluation(model_fn, input_shape, data, labels, lr=0.01, beta_1=0.9, beta_2=0.99, batch_size=32, n_epochs=10, n_folds=5):\n",
        "  accuracies = []\n",
        "  model_save = []\n",
        "  kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
        "\n",
        "  for train_idx, val_idx in kfold.split(data):\n",
        "    model = model_fn(input_shape, lr=lr, beta_1=beta_1, beta_2=beta_2)\n",
        "    x_train, y_train, x_val, y_val = data[train_idx], labels[train_idx], data[val_idx], labels[val_idx]\n",
        "    history = model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
        "    _, acc = model.evaluate(x_val, y_val, verbose=0)\n",
        "    print('Fold accuracy: ', acc*100.0)\n",
        "    # print('Mean accuracy:', np.mean(accuracy))\n",
        "    accuracies.append(acc)\n",
        "    model_save.append(history)\n",
        "  return np.mean(accuracies), accuracies, model_save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZanDtYfUU5A"
      },
      "source": [
        "# k fold training\n",
        "def kfold_training(model_fn, input_shape, data, labels, batch_size=32, n_epochs=10, n_folds=5):\n",
        "  accuracies = []\n",
        "  model_save = []\n",
        "  kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
        "  model = model_fn(input_shape)\n",
        "\n",
        "  for train_idx, val_idx in kfold.split(data):\n",
        "    x_train, y_train, x_val, y_val = data[train_idx], labels[train_idx], data[val_idx], labels[val_idx]\n",
        "    history = model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
        "    accuracy = history.history['val_accuracy']\n",
        "    _, acc = model.evaluate(x_val, y_val, verbose=0)\n",
        "    print('Fold accuracy: ', acc*100.0)\n",
        "    print('Mean val accuracy:', accuracy)\n",
        "    accuracies.append(np.mean(accuracy))\n",
        "    model_save.append(history)\n",
        "  return model, accuracies, model_save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN4v1okpGOvI"
      },
      "source": [
        "# plots accuracy and loss curves for kfold\n",
        "def learning_curves_kfold(model_saves, string):\n",
        "  fig = plt.figure(figsize=(7,3))\n",
        "  fig.suptitle('Loss and Validation Accuracy Curves for \\n' + string, y=1.1)\n",
        "  # plt.xlabel('Epoch')\n",
        "  for i in range(len(model_saves)):\n",
        "    ax_1 = fig.add_subplot(1, 2, 1)\n",
        "    ax_1.set_title('Binary Cross Entropy Loss')\n",
        "    ax_1.plot(model_saves[i].history['loss'], color='blue', label='train')\n",
        "    ax_1.plot(model_saves[i].history['val_loss'], color='orange', label='validation')\n",
        "    ax_1.set_xticks(range(0,10))\n",
        "    \n",
        "    ax_2 = fig.add_subplot(1, 2, 2)\n",
        "    ax_2.set_title('Validation Accuracy')\n",
        "    ax_2.plot(model_saves[i].history['accuracy'], color='blue', label='train')\n",
        "    ax_2.plot(model_saves[i].history['val_accuracy'], color='orange', label='validation')\n",
        "    ax_2.set_xticks(range(0,10))\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4J8GCZRT3xw"
      },
      "source": [
        "# plots accuracy and loss curves\n",
        "def learning_curves(model_saves):\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.title('Binary Cross Entropy Loss')\n",
        "  plt.plot(model_saves.history['loss'], color='blue', label='train')\n",
        "  plt.plot(model_saves.history['val_loss'], color='orange', label='validation')\n",
        "  \n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.plot(model_saves.history['accuracy'], color='blue', label='train')\n",
        "  plt.plot(model_saves.history['val_accuracy'], color='orange', label='validation')\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iSPju6qGRsA"
      },
      "source": [
        "# box plot of accuracy with std\n",
        "def performance_plot(scores):\n",
        "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
        "\tplt.boxplot(scores)\n",
        "\tplt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Ipa8nmWQIe"
      },
      "source": [
        "## IMAGE AUGMENTATION\n",
        "datagen1 = ImageDataGenerator(shear_range=0.3)\n",
        "datagen2 = ImageDataGenerator(\n",
        "        rotation_range=10,  \n",
        "        zoom_range = 0.10,  \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1)\n",
        "\n",
        "adaptive_lr = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMcgKCDKDevk"
      },
      "source": [
        "def results_to_csv_string(results, filename):\n",
        "  as_strings = []\n",
        "  for r in results:\n",
        "    a_s = [str(i) for i in r]\n",
        "    s = ''.join(a_s)\n",
        "    as_strings.append(s)\n",
        "\n",
        "  submissions = pd.Series(as_strings, name=\"Label\")\n",
        "  submission = pd.concat([pd.Series(range(0, 14000), name=\"Id\"), submissions], axis=1)\n",
        "  submission.to_csv('/content/drive/MyDrive/' + filename + '.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx5lBVS4Gcd1"
      },
      "source": [
        "# Original Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mPDRPOcGfOt"
      },
      "source": [
        "def model(dropout=0.4, input_shape=(64,64,1)):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size= 3, padding='same', activation='relu', input_shape = input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 4, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(55, activation='softmax'))\n",
        "  model.add(Reshape((5, 11)))\n",
        "\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt9I1yX4BZ-e"
      },
      "source": [
        "# original model - original data\n",
        "original_model = model(0.4)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_dataset, train_onehot_labels, shuffle=True, test_size=0.2)\n",
        "history = original_model.fit(train_x, train_y, epochs=10, batch_size=32, validation_data=(test_x, test_y), verbose=1)\n",
        "\n",
        "predictions = original_model.predict(test_dataset)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'original_model-original_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kSpHEcRDnLu"
      },
      "source": [
        "# original model - sheared data\n",
        "original_model = model(0.4)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_dataset, train_onehot_labels, shuffle=True, test_size=0.2)\n",
        "history = original_model.fit(datagen1.flow(train_x, train_y, batch_size=32), epochs=10, steps_per_epoch=train_x.shape[0]//32,\n",
        "                             batch_size=32, validation_data=(test_x, test_y), verbose=1)\n",
        "\n",
        "predictions = original_model.predict(test_dataset)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'original_model-sheared_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p95a4TMwEbQq"
      },
      "source": [
        "# original model - cropped data\n",
        "original_model = model(0.4, (20,64,1))\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(new_cut, train_onehot_labels, shuffle=True, test_size=0.2)\n",
        "history = original_model.fit(train_x, train_y, epochs=10,\n",
        "                             batch_size=32, validation_data=(test_x, test_y), verbose=1)\n",
        "\n",
        "predictions = original_model.predict(new_cut_test)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'original_model-cropped data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3E-z_0YD6bX"
      },
      "source": [
        "# original model - aug2 data\n",
        "original_model = model(0.4)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_dataset, train_onehot_labels, shuffle=True, test_size=0.2)\n",
        "history = original_model.fit(datagen2.flow(train_x, train_y, batch_size=32), epochs=10, steps_per_epoch=train_x.shape[0]//32,\n",
        "                             batch_size=32, validation_data=(test_x, test_y), verbose=1)\n",
        "\n",
        "predictions = original_model.predict(test_dataset)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'original_model-aug2_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3v6GGaeH205"
      },
      "source": [
        "# original model - adaptive learning rate\n",
        "original_model = model(0.4)\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(train_dataset, train_onehot_labels, shuffle=True, test_size=0.2)\n",
        "history = original_model.fit(train_x, train_y, epochs=10, batch_size=32, validation_data=(test_x, test_y), callbacks=[adaptive_lr], verbose=1)\n",
        "\n",
        "predictions = original_model.predict(test_dataset)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'original_model-aug2_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVoxU0f7PejG"
      },
      "source": [
        "model_4 = original_model(0.4, (20, 64, 1))\n",
        "train_x, test_x, train_y, test_y = train_test_split(new_cut, train_onehot_labels, shuffle=True, test_size=0.2)\n",
        "history = model_4.fit(datagen1.flow(train_x, train_y, batch_size=32), epochs=5, validation_data=(datagen.flow(test_x, test_y, batch_size=32)))\n",
        "\n",
        "predictions = model_4.predict(new_cut_test)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'result_shear_cut')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFmEFw_tGsBF"
      },
      "source": [
        "# Dropout test\n",
        "kfold = KFold(5, shuffle=True, random_state = 1)\n",
        "epochs = 5\n",
        "dropout_val = 0.1\n",
        "\n",
        "\n",
        "accuracies = []\n",
        "for train, test in kfold.split(train_dataset):\n",
        "  test_model = test_drop_model(dropout_val)\n",
        "  x_train, y_train, x_val, y_val = train_dataset[train], train_onehot_labels[train], train_dataset[test], train_onehot_labels[test]\n",
        "  history = test_model.fit(x_train, y_train, epochs = epochs, batch_size = 32, validation_data=(x_val, y_val), verbose = 1)\n",
        "  _, acc = test_model.evaluate(x_val, y_val, verbose=0)\n",
        "  print('Fold accuracy: ', acc*100.0)\n",
        "  accuracies.append(acc)\n",
        "  dropout_val = dropout_val + 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4rrq5mgH5Zd"
      },
      "source": [
        "# Small LR-Adam Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucyh9jP9H_vs"
      },
      "source": [
        "def lr_adam_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size= 3, activation='relu', input_shape = (64,64,1)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(55, activation='softmax'))\n",
        "  model.add(Reshape((5, 11)))\n",
        "\n",
        "  optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NedfCiy8IQK1"
      },
      "source": [
        "# Max Pooling Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqWA5v7_ISpq"
      },
      "source": [
        "def pooling_model(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size= 3, activation='relu', input_shape = input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Conv2D(32, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "  model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
        "\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 4, activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(55, activation='softmax'))\n",
        "  model.add(Reshape((5, 11)))\n",
        "  opt = SGD(lr=0.01, momentum=0.9)\n",
        "  model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rm-DwA-RPGN"
      },
      "source": [
        "model_2 = pooling_model((64, 64, 1))\n",
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(train_dataset, train_onehot_labels, test_size = 0.25)\n",
        "history = model_2.fit(train_x2, train_y2, epochs=10, batch_size = 32, validation_data=(test_x2, test_y2))\n",
        "\n",
        "predictions = model_2.predict(test_dataset)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'pool_model-original_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYZiuxKDHw1O"
      },
      "source": [
        "# Medium-Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtCJueSHywS"
      },
      "source": [
        "def medium_deep_model(input_shape, lr=0.01, beta_1=0.9, beta_2=0.99, amsgrad=False):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu', input_shape=input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(128, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(128, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(32, kernel_size = 4, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(55, activation='softmax'))\n",
        "  model.add(Reshape((5, 11)))\n",
        "\n",
        "  opt = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, amsgrad=amsgrad)\n",
        "  model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv6OE3u6Vu_O"
      },
      "source": [
        "# tuning for betas\n",
        "betas = [0.5, 0.75, 0.8, 0.9, 0.93, 0.95, 0.99]\n",
        "b_accuracies = []\n",
        "b_meanvalacc = []\n",
        "b_histories = []\n",
        "\n",
        "for beta in betas:\n",
        "  print('-----------> evaluating b = ', beta)\n",
        "  acc, accuracies, histories = kfold_evaluation(medium_deep_model, beta, (20,64,1), new_cut, train_onehot_labels, n_epochs=10)\n",
        "  b_accuracies.append(accuracies)\n",
        "  b_meanvalacc.append(acc)\n",
        "  b_histories.append(histories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDBr5vPZV9uT"
      },
      "source": [
        "lr_val_losses = []\n",
        "for hist in lr_histories:\n",
        "  lr_val_losses.append(np.mean(hist[0].history['loss']))\n",
        "plt.title('Average Fold Loss for Betas')\n",
        "plt.plot(betas, lr_val_losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwKEDgTyWJev"
      },
      "source": [
        "plt.title('Average Fold Validation Accuracy for Betas')\n",
        "plt.plot(learning_rates, lr_val_losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YePfrVZsVmM6"
      },
      "source": [
        "# tuning for learning rate\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "lr_accuracies = []\n",
        "lr_meanvalacc = []\n",
        "lr_histories = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "  print('-----------> evaluating lr = ', lr)\n",
        "  acc, accuracies, histories = kfold_evaluation_lr(medium_deep_model, lr, (20,64,1), new_cut, train_onehot_labels, n_epochs=10)\n",
        "  lr_accuracies.append(accuracies)\n",
        "  lr_meanvalacc.append(acc)\n",
        "  lr_histories.append(histories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0fvWRCdWAlx"
      },
      "source": [
        "lr_val_losses = []\n",
        "for hist in lr_histories1:\n",
        "  lr_val_losses.append(np.mean(hist[0].history['loss']))\n",
        "\n",
        "plt.title('Average Fold Loss for Learning Rates')\n",
        "plt.plot(learning_rates1, lr_val_losses)\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaOq7tbRWGdP"
      },
      "source": [
        "plt.title('Average Fold Validation Accuracy for Learning Rates')\n",
        "plt.plot(learning_rates1, lr_val_losses)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5Y0slFIV6e"
      },
      "source": [
        "# new_cut dataset no change kfold training - 98.9\n",
        "model_k, accuracies, histories = kfold_training(medium_deep_model, (20, 64, 1), new_cut, train_onehot_labels, n_epochs=10)\n",
        "\n",
        "predictions = model_k.predict(new_cut_test)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'medium_deep_cut_kfold')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN8dzvsISrsz"
      },
      "source": [
        "# new_cut dataset no change - 99.67 val acc, submitted 98 test acc\n",
        "model_2 = medium_deep_model((20, 64, 1))\n",
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(new_cut, train_onehot_labels, test_size = 0.25)\n",
        "history = model_2.fit(train_x2, train_y2, epochs=10, batch_size = 32, validation_data=(test_x2, test_y2), callbacks=[early, checkpoint])\n",
        "\n",
        "predictions = model_2.predict(new_cut_test)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'medium_deep-cut')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZcJEvFNW9Hb"
      },
      "source": [
        "# new_cut and shearing - \n",
        "datagen = ImageDataGenerator(shear_range = 0.3)\n",
        "\n",
        "# model_4 = medium_deep_model((20, 64, 1))\n",
        "train_x, test_x, train_y, test_y = train_test_split(new_cut, train_onehot_labels, shuffle=True, test_size=0.25)\n",
        "history = model_4.fit(datagen.flow(train_x, train_y, batch_size=32), epochs=5, validation_data=(test_x, test_y))\n",
        "\n",
        "predictions = model_4.predict(new_cut_test)\n",
        "result = evaluate(predictions)\n",
        "\n",
        "results_to_csv(result, 'result_deep_shear_cut_pred')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7dSuXHtI11W"
      },
      "source": [
        "# OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldSumkQ2I4RN"
      },
      "source": [
        "individual_numbers = []\n",
        "individual_targets = []\n",
        "\n",
        "for i in range(56000):\n",
        "  original_img = train_dataset[i].copy()\n",
        "  ret, thresh = cv2.threshold(train_dataset[i].copy(), 23, 255, cv2.THRESH_BINARY)\n",
        "  contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  contours.sort(key=lambda ct: cv2.boundingRect(ct)[0])\n",
        "\n",
        "  j = 0\n",
        "  for c in contours:\n",
        "    if j > 4:\n",
        "      continue\n",
        "    x, y, width, height = cv2.boundingRect(c)\n",
        "    if width < 4 and height < 4:\n",
        "      continue\n",
        "\n",
        "    segmented_digit = original_img[y:y+height, x:x+width]\n",
        "\n",
        "    resized_segmented_digit = cv2.resize(segmented_digit, (18, 18))\n",
        "    padded_digit = np.pad(resized_segmented_digit, ((5,5),(5,5)), \"constant\", constant_values=0) # This part is to add 5 pixels in 4 directions to make it 28x28\n",
        "    individual_numbers.append(padded_digit)\n",
        "    individual_targets.append(train_labels[i][j])\n",
        "\n",
        "    j+=1\n",
        "\n",
        "train, test = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "new_train_dataset = np.concatenate([np.array(individual_numbers), train[0], test[0]])\n",
        "train_onehot_labels = to_categorical(individual_targets)\n",
        "new_train_labels = np.array(train_onehot_labels)\n",
        "\n",
        "new_labels = []\n",
        "for onehot in new_train_labels:\n",
        "  new_labels.append(onehot[:-1])\n",
        "new_train_labels = np.array(new_labels)\n",
        "\n",
        "new_train_labels = np.concatenate([new_train_labels, np.array(to_categorical(train[1])), np.array(to_categorical(test[1]))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbMR9KaoJn26"
      },
      "source": [
        "# Open CV CNN model\n",
        "def cv_model(input_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size= 3, activation='relu', input_shape=input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size= 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTbov6lgJrOT"
      },
      "source": [
        "# pre-process test dataset so that it fits our model\n",
        "individual_numbers_test = []\n",
        "batches_length = []\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "  original_img = test_dataset[i].copy()\n",
        "  ret, thresh = cv2.threshold(test_dataset[i].copy(), 23, 255, cv2.THRESH_BINARY)\n",
        "  contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  contours.sort(key=lambda ct: cv2.boundingRect(ct)[0])\n",
        "\n",
        "  j = 0\n",
        "  for c in contours:\n",
        "    if j > 4:\n",
        "      continue\n",
        "    x, y, width, height = cv2.boundingRect(c)\n",
        "    if width < 4 and height < 4:\n",
        "      continue\n",
        "\n",
        "    segmented_digit = original_img[y:y+height, x:x+width]\n",
        "\n",
        "    resized_segmented_digit = cv2.resize(segmented_digit, (18, 18))\n",
        "    padded_digit = np.pad(resized_segmented_digit, ((5,5),(5,5)), \"constant\", constant_values=0) # This part is to add 5 pixels in 4 directions to make it 28x28\n",
        "    individual_numbers_test.append(padded_digit)\n",
        "\n",
        "    j+=1\n",
        "  batches_length.append(j)\n",
        "\n",
        "new_test_dataset = np.array(individual_numbers_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5nPzMXjJx6k"
      },
      "source": [
        "# predictions and output to csv\n",
        "def evaluate(yhat):\n",
        "  results = []\n",
        "  for y in yhat:\n",
        "    results.append(np.argmax(y))\n",
        "  return results\n",
        "\n",
        "new_train_dataset = new_train_dataset.reshape(new_train_dataset.shape[0],new_train_dataset.shape[1], new_train_dataset.shape[2], 1)\n",
        "cv_mod = cv_model((28,28,1))\n",
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(new_train_dataset, new_train_labels, test_size = 0.10)\n",
        "history = cv_mod.fit(train_x2, train_y2, epochs=15, batch_size = 32, validation_data=(test_x2, test_y2))\n",
        "predictions = cv_mod.predict(new_test_dataset.reshape(new_test_dataset.shape[0],new_test_dataset.shape[1], new_test_dataset.shape[2], 1))\n",
        "results = evaluate(predictions)\n",
        "\n",
        "concatenated_predictions = []\n",
        "i = 0\n",
        "for bl in batches_length:\n",
        "  single_prediction = []\n",
        "  for j in range(i, i + bl):\n",
        "    single_prediction.append(results[j])\n",
        "  for k in range(len(single_prediction), 5):\n",
        "    single_prediction.append(10)\n",
        "  concatenated_predictions.append(single_prediction)\n",
        "  i = i + bl\n",
        "\n",
        "results = pd.Series(concatenated_predictions, name=\"Label\")\n",
        "submission = pd.concat([pd.Series(range(0, 14000), name=\"Id\"), results], axis=1)\n",
        "submission.to_csv('/content/drive/MyDrive/openCV_og.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR91NjF2MigJ"
      },
      "source": [
        "# Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFPqBJ8dMYDc"
      },
      "source": [
        "def deep_model(input_shape, lr=0.01, beta_1=0.9, beta_2=0.99, amsgrad=False):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu', input_shape=input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(128, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(128, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(128, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(256, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(256, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(256, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(512, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(5125, kernel_size = 3, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Conv2D(512, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Conv2D(32, kernel_size = 4, padding='same', activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(55, activation='softmax'))\n",
        "  model.add(Reshape((5, 11)))\n",
        "\n",
        "  opt = Adam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2, amsgrad=amsgrad)\n",
        "  model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etzaSYWTW2Qc"
      },
      "source": [
        "##Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmi0YsN3MsgK"
      },
      "source": [
        "datagen = ImageDataGenerator(shear_range=0.3,\n",
        "                             data_format='channels_last',\n",
        "                             fill_mode='nearest')\n",
        "img = test_dataset[10]\n",
        "img = img.reshape((1, 64, 64, 1))\n",
        "aug_iter = datagen.flow(img, batch_size=32)\n",
        "# datagen_visualize(aug_iter)\n",
        "# bat = next(aug_iter)\n",
        "fig, ax = plt.subplots(nrows= 1, ncols=9, figsize=(15,15))\n",
        "for i in range(9):\n",
        "  image = next(aug_iter)[0] #.astype('uint8')\n",
        "  ax[i].imshow(image)\n",
        "  ax[i].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}